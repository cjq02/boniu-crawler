#!/usr/bin/env python3
"""
åšç‰›çˆ¬è™«æ¼”ç¤ºè„šæœ¬
å±•ç¤ºé‡æ„åé¡¹ç›®çš„ä½¿ç”¨æ–¹æ³•
"""

import sys
import os
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
src_path = Path(__file__).parent.parent / "src"
sys.path.insert(0, str(src_path))

from crawler.sites.boniu.crawler import BoniuCrawler
from crawler.utils.storage import save_data


def demo_basic_usage():
    """æ¼”ç¤ºåŸºæœ¬ä½¿ç”¨æ–¹æ³•"""
    print("=== åšç‰›çˆ¬è™«åŸºæœ¬ä½¿ç”¨æ¼”ç¤º ===")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = BoniuCrawler()
    
    # çˆ¬å–è®ºå›å¸–å­
    print("å¼€å§‹çˆ¬å–åšç‰›ç¤¾åŒºè®ºå›å¸–å­...")
    posts = crawler.crawl_forum_posts()
    
    if posts:
        print(f"âœ… æˆåŠŸè·å– {len(posts)} ä¸ªå¸–å­")
        
        # æ˜¾ç¤ºå‰3ä¸ªå¸–å­çš„ä¿¡æ¯
        print("\nğŸ“‹ å¸–å­é¢„è§ˆ:")
        for i, post in enumerate(posts[:3], 1):
            print(f"{i}. {post['title']}")
            print(f"   ç”¨æˆ·: {post['username']}")
            print(f"   æ—¶é—´: {post['publish_time']}")
            print(f"   å›å¤: {post['reply_count']}, æµè§ˆ: {post['view_count']}")
            print(f"   åˆ†ç±»: {post['category']}")
            print(f"   ç½®é¡¶: {'æ˜¯' if post['is_sticky'] else 'å¦'}, ç²¾å: {'æ˜¯' if post['is_essence'] else 'å¦'}")
            print()
        
        # ä¿å­˜æ•°æ®
        output_file = "data/processed/demo_posts.json"
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        file_path = save_data(posts, os.path.basename(output_file), 
                            output_dir=os.path.dirname(output_file))
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {file_path}")
        
    else:
        print("âŒ æœªè·å–åˆ°ä»»ä½•å¸–å­æ•°æ®")


def demo_advanced_usage():
    """æ¼”ç¤ºé«˜çº§ä½¿ç”¨æ–¹æ³•"""
    print("\n=== é«˜çº§ä½¿ç”¨æ¼”ç¤º ===")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = BoniuCrawler()
    
    # è·å–çˆ¬è™«çŠ¶æ€
    status = crawler.get_status()
    print(f"ğŸ“Š çˆ¬è™«çŠ¶æ€: {status['name']}")
    print(f"   è¿è¡ŒçŠ¶æ€: {'è¿è¡Œä¸­' if status['is_running'] else 'å·²åœæ­¢'}")
    
    # çˆ¬å–æ•°æ®
    posts = crawler.crawl_forum_posts()
    
    if posts:
        # æ•°æ®åˆ†æ
        total_posts = len(posts)
        sticky_posts = len([p for p in posts if p['is_sticky']])
        essence_posts = len([p for p in posts if p['is_essence']])
        categories = {}
        
        for post in posts:
            category = post['category'] or 'æœªåˆ†ç±»'
            categories[category] = categories.get(category, 0) + 1
        
        print(f"\nğŸ“ˆ æ•°æ®åˆ†æ:")
        print(f"   æ€»å¸–å­æ•°: {total_posts}")
        print(f"   ç½®é¡¶å¸–å­: {sticky_posts}")
        print(f"   ç²¾åå¸–å­: {essence_posts}")
        print(f"   åˆ†ç±»ç»Ÿè®¡:")
        for category, count in categories.items():
            print(f"     {category}: {count}")
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis = {
            "summary": {
                "total_posts": total_posts,
                "sticky_posts": sticky_posts,
                "essence_posts": essence_posts,
                "categories": categories
            },
            "posts": posts
        }
        
        output_file = "data/processed/demo_analysis.json"
        file_path = save_data(analysis, os.path.basename(output_file),
                            output_dir=os.path.dirname(output_file))
        print(f"ğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {file_path}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ åšç‰›çˆ¬è™«é‡æ„ç‰ˆæ¼”ç¤º")
    print("=" * 50)
    
    try:
        # åŸºæœ¬ä½¿ç”¨æ¼”ç¤º
        demo_basic_usage()
        
        # é«˜çº§ä½¿ç”¨æ¼”ç¤º
        demo_advanced_usage()
        
        print("\nâœ… æ¼”ç¤ºå®Œæˆï¼")
        print("ğŸ“š æ›´å¤šä¿¡æ¯è¯·æŸ¥çœ‹ docs/ ç›®å½•ä¸‹çš„æ–‡æ¡£")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
