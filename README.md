# Boniu Crawler

ä¸€ä¸ªåŸºäºPythonçš„é«˜æ€§èƒ½çˆ¬è™«é¡¹ç›®ï¼Œæ”¯æŒå¤šç§çˆ¬å–æ–¹å¼å’Œæ•°æ®å­˜å‚¨æ ¼å¼ã€‚

## ğŸš€ åŠŸèƒ½ç‰¹æ€§

- **å¤šç§çˆ¬å–æ–¹å¼**: æ”¯æŒRequestsã€Seleniumã€Scrapyã€Playwrightç­‰
- **é«˜æ€§èƒ½å¼‚æ­¥çˆ¬å–**: åŸºäºasyncioçš„å¼‚æ­¥å¹¶å‘çˆ¬å–
- **çµæ´»æ•°æ®å­˜å‚¨**: æ”¯æŒJSONã€CSVã€Excelã€TXTç­‰å¤šç§å­˜å‚¨æ ¼å¼
- **å¯é…ç½®æ€§å¼º**: æ”¯æŒä»£ç†ã€è¯·æ±‚å¤´ã€å»¶è¿Ÿç­‰é…ç½®
- **è¯¦ç»†æ—¥å¿—è®°å½•**: ä½¿ç”¨Loguruè¿›è¡Œæ—¥å¿—ç®¡ç†
- **åçˆ¬è™«ç­–ç•¥**: å†…ç½®å¤šç§åçˆ¬è™«ç»•è¿‡ç­–ç•¥
- **ä»£ç è´¨é‡**: ä½¿ç”¨Blackã€Flake8ã€MyPyç­‰å·¥å…·ä¿è¯ä»£ç è´¨é‡

## ğŸ“¦ æŠ€æœ¯æ ˆ

- **Python**: 3.8+
- **Requests**: HTTPå®¢æˆ·ç«¯
- **BeautifulSoup4**: HTMLè§£æ
- **Selenium**: æµè§ˆå™¨è‡ªåŠ¨åŒ–
- **Scrapy**: çˆ¬è™«æ¡†æ¶
- **Playwright**: ç°ä»£æµè§ˆå™¨è‡ªåŠ¨åŒ–
- **Pandas**: æ•°æ®å¤„ç†
- **Loguru**: æ—¥å¿—ç®¡ç†
- **Pydantic**: æ•°æ®éªŒè¯
- **aiohttp**: å¼‚æ­¥HTTPå®¢æˆ·ç«¯

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
boniu-crawler/
â”œâ”€â”€ main.py                 # ä¸»ç¨‹åºå…¥å£
â”œâ”€â”€ config.py               # é…ç½®ç®¡ç†
â”œâ”€â”€ logger.py               # æ—¥å¿—ç®¡ç†
â”œâ”€â”€ utils.py                # å·¥å…·å‡½æ•°
â”œâ”€â”€ crawler.py              # åŸºç¡€çˆ¬è™«ç±»
â”œâ”€â”€ requests_crawler.py     # Requestsçˆ¬è™«
â”œâ”€â”€ requirements.txt        # ä¾èµ–æ–‡ä»¶
â”œâ”€â”€ pyproject.toml          # é¡¹ç›®é…ç½®
â”œâ”€â”€ .env.example            # ç¯å¢ƒå˜é‡ç¤ºä¾‹
â”œâ”€â”€ data/                   # æ•°æ®å­˜å‚¨ç›®å½•
â”œâ”€â”€ logs/                   # æ—¥å¿—æ–‡ä»¶ç›®å½•
â”œâ”€â”€ tests/                  # æµ‹è¯•æ–‡ä»¶
â””â”€â”€ README.md
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### é…ç½®ç¯å¢ƒ

```bash
# å¤åˆ¶ç¯å¢ƒå˜é‡ç¤ºä¾‹æ–‡ä»¶
cp .env.example .env

# ç¼–è¾‘é…ç½®æ–‡ä»¶
# æ ¹æ®éœ€è¦ä¿®æ”¹ .env æ–‡ä»¶ä¸­çš„é…ç½®
```

### è¿è¡Œç¤ºä¾‹

```bash
# çˆ¬å–æ–°é—»ç½‘ç«™
python main.py news

# çˆ¬å–APIæ•°æ®
python main.py api

# æ‰¹é‡çˆ¬å–
python main.py batch

# åˆ†é¡µçˆ¬å–
python main.py paginated

# å¼‚æ­¥çˆ¬å–
python main.py async

# è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
python main.py all
```

## ğŸ“– ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€çˆ¬è™«ä½¿ç”¨

```python
from requests_crawler import RequestsCrawler

# åˆ›å»ºçˆ¬è™«å®ä¾‹
crawler = RequestsCrawler("my_crawler")

# çˆ¬å–å•ä¸ªURL
result = crawler.crawl_url("https://example.com")
print(result.data)

# çˆ¬å–HTMLé¡µé¢
selectors = {
    "title": "h1",
    "content": ".content",
    "links": "a",
}
result = crawler.crawl_html("https://example.com", selectors)
print(result.data)

# çˆ¬å–APIæ•°æ®
result = crawler.crawl_api("https://api.example.com/data")
print(result.data)
```

### æ‰¹é‡çˆ¬å–

```python
urls = [
    "https://api.example.com/data1",
    "https://api.example.com/data2",
    "https://api.example.com/data3",
]

crawler = RequestsCrawler()
results = crawler.batch_crawl(urls)

for result in results:
    if not result.error:
        print(f"æˆåŠŸ: {result.url}")
    else:
        print(f"å¤±è´¥: {result.url} - {result.error}")
```

### åˆ†é¡µçˆ¬å–

```python
crawler = RequestsCrawler()

page_config = {
    "start_page": 1,
    "max_pages": 10,
    "page_param": "page",
    "data_selector": ".item",
}

data = crawler.crawl_paginated("https://example.com/list", page_config)
print(f"è·å–åˆ° {len(data)} é¡µæ•°æ®")
```

### å¼‚æ­¥çˆ¬å–

```python
import asyncio
from requests_crawler import AsyncRequestsCrawler

async def main():
    crawler = AsyncRequestsCrawler()
    
    urls = [
        "https://api.example.com/data1",
        "https://api.example.com/data2",
    ]
    
    results = await crawler.batch_crawl(urls)
    
    for result in results:
        if not result.error:
            print(f"æˆåŠŸ: {result.url}")

# è¿è¡Œå¼‚æ­¥çˆ¬è™«
asyncio.run(main())
```

### è‡ªå®šä¹‰çˆ¬è™«

```python
from crawler import BaseCrawler
from requests_crawler import RequestsCrawler

class MyCrawler(RequestsCrawler):
    def __init__(self):
        super().__init__("my_crawler")
    
    def run(self):
        """å®ç°å…·ä½“çš„çˆ¬å–é€»è¾‘"""
        url = "https://example.com"
        result = self.crawl_url(url)
        
        if not result.error:
            # å¤„ç†æ•°æ®
            processed_data = self.process_data(result.data)
            # ä¿å­˜æ•°æ®
            self.save_data(processed_data, "my_data.json")
    
    def process_data(self, data):
        """å¤„ç†çˆ¬å–åˆ°çš„æ•°æ®"""
        # åœ¨è¿™é‡Œæ·»åŠ æ•°æ®å¤„ç†é€»è¾‘
        return data

# è¿è¡Œè‡ªå®šä¹‰çˆ¬è™«
crawler = MyCrawler()
crawler.start()
```

## âš™ï¸ é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡é…ç½®

åˆ›å»º `.env` æ–‡ä»¶å¹¶é…ç½®ä»¥ä¸‹å‚æ•°ï¼š

```bash
# åº”ç”¨é…ç½®
APP_NAME=Boniu Crawler
APP_VERSION=1.0.0
ENVIRONMENT=development
DEBUG=false

# çˆ¬è™«é…ç½®
CRAWLER__TIMEOUT=30
CRAWLER__RETRIES=3
CRAWLER__RETRY_DELAY=1
CRAWLER__MAX_CONCURRENT=5

# ä»£ç†é…ç½®
PROXY__ENABLED=false
PROXY__HOST=127.0.0.1
PROXY__PORT=8080
PROXY__USERNAME=
PROXY__PASSWORD=

# æ—¥å¿—é…ç½®
LOGGING__LEVEL=INFO
LOGGING__FILE=./logs/crawler.log
LOGGING__ROTATION=1 day
LOGGING__RETENTION=30 days

# å­˜å‚¨é…ç½®
STORAGE__OUTPUT_DIR=./data
STORAGE__FORMAT=json
STORAGE__FILENAME=crawled_data

# åçˆ¬è™«é…ç½®
ANTI_CRAWLER__ENABLED=true
ANTI_CRAWLER__RANDOM_DELAY=true
ANTI_CRAWLER__DELAY_RANGE=1,3
ANTI_CRAWLER__ROTATE_USER_AGENTS=true
```

### åŸºç¡€é…ç½®

- `timeout`: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
- `retries`: é‡è¯•æ¬¡æ•°
- `retry_delay`: é‡è¯•å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
- `max_concurrent`: æœ€å¤§å¹¶å‘æ•°

### åçˆ¬è™«é…ç½®

- `random_delay`: æ˜¯å¦å¯ç”¨éšæœºå»¶è¿Ÿ
- `delay_range`: å»¶è¿Ÿæ—¶é—´èŒƒå›´ï¼ˆç§’ï¼‰
- `rotate_user_agents`: æ˜¯å¦è½®æ¢User-Agent
- `use_proxy_pool`: æ˜¯å¦ä½¿ç”¨ä»£ç†æ± 

## ğŸ“Š æ•°æ®å­˜å‚¨

æ”¯æŒå¤šç§æ•°æ®å­˜å‚¨æ ¼å¼ï¼š

- **JSON**: é€‚åˆç»“æ„åŒ–æ•°æ®
- **CSV**: é€‚åˆè¡¨æ ¼æ•°æ®
- **Excel**: é€‚åˆå¤æ‚è¡¨æ ¼æ•°æ®
- **TXT**: é€‚åˆçº¯æ–‡æœ¬æ•°æ®

## ğŸ“ æ—¥å¿—ç®¡ç†

ä½¿ç”¨Loguruè¿›è¡Œæ—¥å¿—ç®¡ç†ï¼Œæ”¯æŒï¼š

- æ§åˆ¶å°è¾“å‡º
- æ–‡ä»¶è®°å½•
- é”™è¯¯è¿½è¸ª
- æ€§èƒ½ç›‘æ§
- æ—¥å¿—è½®è½¬

## ğŸ§ª æµ‹è¯•

é¡¹ç›®åŒ…å«å®Œæ•´çš„å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•ï¼š

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest tests/test_crawler.py

# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest --cov=. --cov-report=html
```

## ğŸ”§ å¼€å‘å·¥å…·

### ä»£ç æ ¼å¼åŒ–

```bash
# æ ¼å¼åŒ–ä»£ç 
black .

# æ’åºå¯¼å…¥
isort .

# ç±»å‹æ£€æŸ¥
mypy .
```

### ä»£ç æ£€æŸ¥

```bash
# ä»£ç é£æ ¼æ£€æŸ¥
flake8 .

# è¿è¡Œæµ‹è¯•
pytest
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. Fork é¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. æ‰“å¼€ Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## âš ï¸ æ³¨æ„äº‹é¡¹

- è¯·éµå®ˆç½‘ç«™çš„ robots.txt è§„åˆ™
- åˆç†è®¾ç½®çˆ¬å–é¢‘ç‡ï¼Œé¿å…å¯¹ç›®æ ‡ç½‘ç«™é€ æˆå‹åŠ›
- æ³¨æ„æ•°æ®ä½¿ç”¨çš„æ³•å¾‹åˆè§„æ€§
- å»ºè®®ä½¿ç”¨ä»£ç†æ± é¿å…IPè¢«å°
- éµå®ˆç›®æ ‡ç½‘ç«™çš„ä½¿ç”¨æ¡æ¬¾

## ğŸ”§ å¼€å‘ç¯å¢ƒ

- **Python**: 3.8+
- **IDE**: æ¨èä½¿ç”¨ PyCharm æˆ– VS Code
- **ç‰ˆæœ¬æ§åˆ¶**: Git
- **åŒ…ç®¡ç†**: pip

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [Python å®˜æ–¹æ–‡æ¡£](https://docs.python.org/)
- [Requests æ–‡æ¡£](https://requests.readthedocs.io/)
- [BeautifulSoup4 æ–‡æ¡£](https://www.crummy.com/software/BeautifulSoup/)
- [Selenium æ–‡æ¡£](https://selenium-python.readthedocs.io/)
- [Scrapy æ–‡æ¡£](https://docs.scrapy.org/)
- [Loguru æ–‡æ¡£](https://loguru.readthedocs.io/)
- [Pydantic æ–‡æ¡£](https://pydantic-docs.helpmanual.io/)
