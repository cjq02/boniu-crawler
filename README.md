# Boniu Crawler

ä¸€ä¸ªä¸“é—¨ç”¨äºçˆ¬å–åšç‰›ç¤¾åŒºè®ºå›çš„Pythonçˆ¬è™«é¡¹ç›®ï¼Œæ”¯æŒè·å–å¸–å­æ ‡é¢˜ã€å›¾ç‰‡ã€ç”¨æˆ·ä¿¡æ¯ã€å‘è¡¨æ—¶é—´ç­‰å®Œæ•´æ•°æ®ã€‚

## ğŸš€ åŠŸèƒ½ç‰¹æ€§

### ğŸ¯ åšç‰›ç¤¾åŒºä¸“é—¨çˆ¬è™«
- **è®ºå›å¸–å­çˆ¬å–**: ä¸“é—¨çˆ¬å–åšç‰›ç¤¾åŒºè®ºå› (https://bbs.boniu123.cc/forum-89-1.html)
- **å®Œæ•´å¸–å­ä¿¡æ¯**: è·å–æ ‡é¢˜ã€URLã€IDã€ç”¨æˆ·åã€å¤´åƒã€å‘å¸–æ—¶é—´ã€å›å¤æ•°ã€æµè§ˆæ•°
- **å›¾ç‰‡æå–**: è‡ªåŠ¨æå–å¸–å­ä¸­çš„å›¾ç‰‡é“¾æ¥
- **åˆ†ç±»è¯†åˆ«**: è‡ªåŠ¨è¯†åˆ«å¸–å­åˆ†ç±»ï¼ˆæ¸¸æˆåŒ…ç½‘ã€æ¸¸æˆAPIã€æ”¯ä»˜æ¸ é“ç­‰ï¼‰
- **ç‰¹æ®Šæ ‡è¯†**: è¯†åˆ«ç½®é¡¶å¸–å­å’Œç²¾åå¸–å­

### ğŸ› ï¸ æŠ€æœ¯ç‰¹æ€§
- **æ¨¡å—åŒ–è®¾è®¡**: åŸºäºåŒ…ç»“æ„çš„å¯æ‰©å±•æ¶æ„
- **å¤šç§çˆ¬å–æ–¹å¼**: æ”¯æŒRequestsã€Seleniumã€Scrapyç­‰
- **é«˜æ€§èƒ½çˆ¬å–**: æ”¯æŒå¼‚æ­¥å¹¶å‘çˆ¬å–
- **çµæ´»æ•°æ®å­˜å‚¨**: æ”¯æŒJSONã€CSVã€Excelç­‰å¤šç§å­˜å‚¨æ ¼å¼
- **å¯é…ç½®æ€§å¼º**: æ”¯æŒä»£ç†ã€è¯·æ±‚å¤´ã€å»¶è¿Ÿç­‰é…ç½®
- **è¯¦ç»†æ—¥å¿—è®°å½•**: ä½¿ç”¨Loguruè¿›è¡Œæ—¥å¿—ç®¡ç†
- **åçˆ¬è™«ç­–ç•¥**: å†…ç½®å¤šç§åçˆ¬è™«ç»•è¿‡ç­–ç•¥
- **ä»£ç è´¨é‡**: ä½¿ç”¨Blackã€Flake8ã€MyPyç­‰å·¥å…·ä¿è¯ä»£ç è´¨é‡

## ğŸ“¦ æŠ€æœ¯æ ˆ

- **Python**: 3.8+
- **Requests**: HTTPå®¢æˆ·ç«¯
- **BeautifulSoup4**: HTMLè§£æ
- **Selenium**: æµè§ˆå™¨è‡ªåŠ¨åŒ–
- **Scrapy**: çˆ¬è™«æ¡†æ¶
- **Playwright**: ç°ä»£æµè§ˆå™¨è‡ªåŠ¨åŒ–
- **Pandas**: æ•°æ®å¤„ç†
- **Loguru**: æ—¥å¿—ç®¡ç†
- **Pydantic**: æ•°æ®éªŒè¯
- **aiohttp**: å¼‚æ­¥HTTPå®¢æˆ·ç«¯

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
boniu-crawler/
â”œâ”€â”€ main.py                     # ä¸»ç¨‹åºå…¥å£ï¼ˆCLIï¼‰
â”œâ”€â”€ boniu_crawler.py            # åšç‰›ç¤¾åŒºçˆ¬è™«ï¼ˆæ­£å¼ç‰ˆï¼‰
â”œâ”€â”€ crawler.py                  # åŸºç¡€çˆ¬è™«ç±»
â”œâ”€â”€ requests_crawler.py          # Requestsçˆ¬è™«å®ç°
â”œâ”€â”€ config.py                   # é…ç½®ç®¡ç†
â”œâ”€â”€ logger.py                   # æ—¥å¿—ç®¡ç†
â”œâ”€â”€ utils.py                    # å·¥å…·å‡½æ•°
â”œâ”€â”€ crawler_pkg/                # çˆ¬è™«åŒ…
â”‚   â”œâ”€â”€ __init__.py             # åŒ…å…¥å£
â”‚   â”œâ”€â”€ core/                   # æ ¸å¿ƒæ¨¡å—
â”‚   â”‚   â”œâ”€â”€ base.py             # åŸºç¡€çˆ¬è™«ç±»
â”‚   â”‚   â””â”€â”€ requests_impl.py    # Requestså®ç°
â”‚   â””â”€â”€ site_boniu/             # åšç‰›ç«™ç‚¹çˆ¬è™«
â”‚       â””â”€â”€ crawler.py          # åšç‰›çˆ¬è™«å®ç°
â”œâ”€â”€ data/                       # æ•°æ®å­˜å‚¨ç›®å½•
â”‚   â”œâ”€â”€ boniu_forum_posts.json  # è®ºå›å¸–å­æ•°æ®
â”‚   â”œâ”€â”€ boniu_enhanced_posts.json # å¢å¼ºç‰ˆå¸–å­æ•°æ®
â”‚   â””â”€â”€ test_posts.json         # æµ‹è¯•æ•°æ®
â”œâ”€â”€ logs/                       # æ—¥å¿—æ–‡ä»¶ç›®å½•
â”œâ”€â”€ tests/                      # æµ‹è¯•æ–‡ä»¶ç›®å½•
â”œâ”€â”€ test_boniu_crawler.py       # åšç‰›çˆ¬è™«æµ‹è¯•
â”œâ”€â”€ test_fixed_crawler.py       # ä¿®å¤ç‰ˆçˆ¬è™«æµ‹è¯•
â”œâ”€â”€ test_website.py             # ç½‘ç«™æµ‹è¯•
â”œâ”€â”€ requirements.txt            # ä¾èµ–æ–‡ä»¶
â”œâ”€â”€ pyproject.toml              # é¡¹ç›®é…ç½®
â”œâ”€â”€ env.example                 # ç¯å¢ƒå˜é‡ç¤ºä¾‹
â”œâ”€â”€ BONIU_CRAWLER_README.md     # åšç‰›çˆ¬è™«è¯¦ç»†è¯´æ˜
â”œâ”€â”€ PROJECT_SUMMARY.md          # é¡¹ç›®æ€»ç»“
â””â”€â”€ README.md                   # é¡¹ç›®è¯´æ˜
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ä¸€é”®è¿è¡Œ

```bash
# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd boniu-crawler

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# ç«‹å³å¼€å§‹çˆ¬å–
python main.py
```

### å®‰è£…ä¾èµ–

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

### é…ç½®ç¯å¢ƒ

```bash
# å¤åˆ¶ç¯å¢ƒå˜é‡ç¤ºä¾‹æ–‡ä»¶
cp .env.example .env

# ç¼–è¾‘é…ç½®æ–‡ä»¶
# æ ¹æ®éœ€è¦ä¿®æ”¹ .env æ–‡ä»¶ä¸­çš„é…ç½®
```

### è¿è¡Œç¤ºä¾‹

```bash
# ä½¿ç”¨CLIè¿è¡Œåšç‰›çˆ¬è™«ï¼ˆæ¨èï¼‰
python main.py

# æŒ‡å®šè¾“å‡ºæ–‡ä»¶
python main.py --output data/my_posts.json

# è¿è¡Œåšç‰›çˆ¬è™«ï¼ˆç›´æ¥è¿è¡Œï¼‰
python boniu_crawler.py

# è¿è¡Œæµ‹è¯•è„šæœ¬
python test_boniu_crawler.py

# è¿è¡Œç½‘ç«™æµ‹è¯•
python test_website.py
```

## ğŸ“– ä½¿ç”¨ç¤ºä¾‹

### åšç‰›çˆ¬è™«ä½¿ç”¨

```python
from crawler_pkg import BoniuCrawler

# åˆ›å»ºåšç‰›çˆ¬è™«å®ä¾‹
crawler = BoniuCrawler()

# çˆ¬å–è®ºå›å¸–å­åˆ—è¡¨
posts = crawler.crawl_forum_posts()
print(f"è·å–åˆ° {len(posts)} ä¸ªå¸–å­")

# æŸ¥çœ‹å¸–å­ä¿¡æ¯
for post in posts[:3]:  # æ˜¾ç¤ºå‰3ä¸ªå¸–å­
    print(f"æ ‡é¢˜: {post['title']}")
    print(f"ç”¨æˆ·: {post['username']}")
    print(f"å‘å¸ƒæ—¶é—´: {post['publish_time']}")
    print(f"å›å¤æ•°: {post['reply_count']}")
    print(f"æµè§ˆæ•°: {post['view_count']}")
    print(f"åˆ†ç±»: {post['category']}")
    print(f"æ˜¯å¦ç½®é¡¶: {post['is_sticky']}")
    print(f"æ˜¯å¦ç²¾å: {post['is_essence']}")
    print("---")
```

### æ•°æ®ä¿å­˜

```python
from crawler_pkg import BoniuCrawler
from utils import save_data

# åˆ›å»ºçˆ¬è™«å¹¶çˆ¬å–æ•°æ®
crawler = BoniuCrawler()
posts = crawler.crawl_forum_posts()

# ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶
if posts:
    file_path = save_data(posts, "boniu_posts", output_dir="data")
    print(f"æ•°æ®å·²ä¿å­˜åˆ°: {file_path}")
    
    # ä¹Ÿå¯ä»¥ä½¿ç”¨çˆ¬è™«å†…ç½®çš„ä¿å­˜æ–¹æ³•
    crawler.save_data(posts, "my_posts.json")
```

### å‘½ä»¤è¡Œä½¿ç”¨

```bash
# ä½¿ç”¨main.pyä½œä¸ºCLIå·¥å…·
python main.py --help

# é»˜è®¤è¾“å‡ºåˆ°data/boniu_forum_posts.json
python main.py

# æŒ‡å®šè¾“å‡ºæ–‡ä»¶
python main.py --output data/my_custom_posts.json

# æŸ¥çœ‹è¾“å‡º
python main.py --output data/test_output.json
```

### æµ‹è¯•å’Œè°ƒè¯•

```python
# è¿è¡Œæµ‹è¯•è„šæœ¬
python test_boniu_crawler.py

# è¿è¡Œç½‘ç«™è¿æ¥æµ‹è¯•
python test_website.py

# åœ¨ä»£ç ä¸­è¿›è¡Œæµ‹è¯•
from crawler_pkg import BoniuCrawler

crawler = BoniuCrawler()

# æµ‹è¯•å•ä¸ªå¸–å­è§£æ
test_html = "<html>...</html>"  # ç¤ºä¾‹HTML
# å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æµ‹è¯•é€»è¾‘

# æŸ¥çœ‹æ—¥å¿—
import logging
logging.basicConfig(level=logging.INFO)
```

### æ‰©å±•çˆ¬è™«

```python
from crawler_pkg.site_boniu.crawler import BoniuCrawler

class EnhancedBoniuCrawler(BoniuCrawler):
    def __init__(self):
        super().__init__()
        # æ·»åŠ è‡ªå®šä¹‰é…ç½®
        self.custom_headers = {
            'Custom-Header': 'MyValue'
        }
    
    def crawl_forum_posts(self):
        """é‡å†™çˆ¬å–æ–¹æ³•ï¼Œæ·»åŠ è‡ªå®šä¹‰é€»è¾‘"""
        posts = super().crawl_forum_posts()
        
        # æ·»åŠ è‡ªå®šä¹‰å¤„ç†
        for post in posts:
            post['custom_field'] = self.process_custom_data(post)
        
        return posts
    
    def process_custom_data(self, post):
        """è‡ªå®šä¹‰æ•°æ®å¤„ç†"""
        # åœ¨è¿™é‡Œæ·»åŠ è‡ªå®šä¹‰é€»è¾‘
        return f"processed_{post.get('id', 'unknown')}"

# ä½¿ç”¨æ‰©å±•çˆ¬è™«
crawler = EnhancedBoniuCrawler()
posts = crawler.crawl_forum_posts()
```

## âš™ï¸ é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡é…ç½®

å¤åˆ¶ `env.example` æ–‡ä»¶ä¸º `.env` å¹¶é…ç½®ä»¥ä¸‹å‚æ•°ï¼š

```bash
# åº”ç”¨é…ç½®
APP_NAME=Boniu Crawler
APP_VERSION=1.0.0
ENVIRONMENT=development
DEBUG=false

# åšç‰›çˆ¬è™«é…ç½®
BONIU__BASE_URL=https://bbs.boniu123.cc
BONIU__FORUM_URL=https://bbs.boniu123.cc/forum-89-1.html
BONIU__TIMEOUT=30
BONIU__RETRIES=3
BONIU__RETRY_DELAY=1

# çˆ¬è™«é€šç”¨é…ç½®
CRAWLER__TIMEOUT=30
CRAWLER__RETRIES=3
CRAWLER__RETRY_DELAY=1
CRAWLER__MAX_CONCURRENT=5

# ä»£ç†é…ç½®
PROXY__ENABLED=false
PROXY__HOST=127.0.0.1
PROXY__PORT=8080
PROXY__USERNAME=
PROXY__PASSWORD=

# æ—¥å¿—é…ç½®
LOGGING__LEVEL=INFO
LOGGING__FILE=./logs/crawler.log
LOGGING__ROTATION=1 day
LOGGING__RETENTION=30 days

# å­˜å‚¨é…ç½®
STORAGE__OUTPUT_DIR=./data
STORAGE__FORMAT=json
STORAGE__FILENAME=boniu_forum_posts

# åçˆ¬è™«é…ç½®
ANTI_CRAWLER__ENABLED=true
ANTI_CRAWLER__RANDOM_DELAY=true
ANTI_CRAWLER__DELAY_RANGE=1,3
ANTI_CRAWLER__ROTATE_USER_AGENTS=true
ANTI_CRAWLER__USER_AGENT=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36
```

### åšç‰›çˆ¬è™«é…ç½®

- `BONIU__BASE_URL`: åšç‰›ç¤¾åŒºåŸºç¡€URL
- `BONIU__FORUM_URL`: è®ºå›é¡µé¢URL
- `BONIU__TIMEOUT`: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
- `BONIU__RETRIES`: é‡è¯•æ¬¡æ•°
- `BONIU__RETRY_DELAY`: é‡è¯•å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰

### é€šç”¨çˆ¬è™«é…ç½®

- `CRAWLER__TIMEOUT`: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
- `CRAWLER__RETRIES`: é‡è¯•æ¬¡æ•°
- `CRAWLER__RETRY_DELAY`: é‡è¯•å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰
- `CRAWLER__MAX_CONCURRENT`: æœ€å¤§å¹¶å‘æ•°

### åçˆ¬è™«é…ç½®

- `ANTI_CRAWLER__ENABLED`: æ˜¯å¦å¯ç”¨åçˆ¬è™«ç­–ç•¥
- `ANTI_CRAWLER__RANDOM_DELAY`: æ˜¯å¦å¯ç”¨éšæœºå»¶è¿Ÿ
- `ANTI_CRAWLER__DELAY_RANGE`: å»¶è¿Ÿæ—¶é—´èŒƒå›´ï¼ˆç§’ï¼‰
- `ANTI_CRAWLER__ROTATE_USER_AGENTS`: æ˜¯å¦è½®æ¢User-Agent
- `ANTI_CRAWLER__USER_AGENT`: è‡ªå®šä¹‰User-Agent

## ğŸ“Š æ•°æ®å­˜å‚¨

### æ”¯æŒçš„æ•°æ®æ ¼å¼

- **JSON**: é€‚åˆç»“æ„åŒ–æ•°æ®ï¼ˆé»˜è®¤æ ¼å¼ï¼‰
- **CSV**: é€‚åˆè¡¨æ ¼æ•°æ®
- **Excel**: é€‚åˆå¤æ‚è¡¨æ ¼æ•°æ®
- **TXT**: é€‚åˆçº¯æ–‡æœ¬æ•°æ®

### åšç‰›çˆ¬è™«æ•°æ®å­—æ®µ

çˆ¬è™«è·å–çš„æ¯ä¸ªå¸–å­åŒ…å«ä»¥ä¸‹å­—æ®µï¼š

```json
{
  "id": "å¸–å­ID",
  "title": "å¸–å­æ ‡é¢˜",
  "url": "å¸–å­é“¾æ¥",
  "username": "å‘å¸–ç”¨æˆ·å",
  "avatar_url": "ç”¨æˆ·å¤´åƒé“¾æ¥",
  "publish_time": "å‘å¸ƒæ—¶é—´",
  "reply_count": å›å¤æ•°,
  "view_count": æµè§ˆæ•°,
  "images": ["å¸–å­å›¾ç‰‡é“¾æ¥åˆ—è¡¨"],
  "category": "å¸–å­åˆ†ç±»",
  "is_sticky": æ˜¯å¦ç½®é¡¶,
  "is_essence": æ˜¯å¦ç²¾å,
  "crawl_time": "çˆ¬å–æ—¶é—´"
}
```

### è¾“å‡ºæ–‡ä»¶

- `data/boniu_forum_posts.json`: é»˜è®¤è¾“å‡ºæ–‡ä»¶
- `data/boniu_enhanced_posts.json`: å¢å¼ºç‰ˆæ•°æ®
- `data/test_posts.json`: æµ‹è¯•æ•°æ®

## ğŸ“ æ—¥å¿—ç®¡ç†

ä½¿ç”¨Loguruè¿›è¡Œæ—¥å¿—ç®¡ç†ï¼Œæ”¯æŒï¼š

- æ§åˆ¶å°è¾“å‡º
- æ–‡ä»¶è®°å½•
- é”™è¯¯è¿½è¸ª
- æ€§èƒ½ç›‘æ§
- æ—¥å¿—è½®è½¬

## ğŸ§ª æµ‹è¯•

é¡¹ç›®åŒ…å«å®Œæ•´çš„æµ‹è¯•è„šæœ¬ï¼š

```bash
# è¿è¡Œåšç‰›çˆ¬è™«æµ‹è¯•
python test_boniu_crawler.py

# è¿è¡Œä¿®å¤ç‰ˆçˆ¬è™«æµ‹è¯•
python test_fixed_crawler.py

# è¿è¡Œç½‘ç«™è¿æ¥æµ‹è¯•
python test_website.py

# ä½¿ç”¨pytestè¿è¡Œæµ‹è¯•ï¼ˆå¦‚æœé…ç½®äº†pytestï¼‰
pytest tests/

# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest --cov=. --cov-report=html
```

## ğŸ”§ å¼€å‘å·¥å…·

### ä»£ç æ ¼å¼åŒ–

```bash
# æ ¼å¼åŒ–ä»£ç 
black .

# æ’åºå¯¼å…¥
isort .

# ç±»å‹æ£€æŸ¥
mypy .
```

### ä»£ç æ£€æŸ¥

```bash
# ä»£ç é£æ ¼æ£€æŸ¥
flake8 .

# è¿è¡Œæµ‹è¯•
pytest
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. Fork é¡¹ç›®
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. æ‰“å¼€ Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## âš ï¸ æ³¨æ„äº‹é¡¹

### æ³•å¾‹åˆè§„æ€§
- è¯·éµå®ˆåšç‰›ç¤¾åŒºçš„ä½¿ç”¨æ¡æ¬¾å’Œrobots.txtè§„åˆ™
- æ³¨æ„æ•°æ®ä½¿ç”¨çš„æ³•å¾‹åˆè§„æ€§ï¼Œä»…ç”¨äºå­¦ä¹ å’Œç ”ç©¶ç›®çš„
- ä¸è¦å°†çˆ¬å–çš„æ•°æ®ç”¨äºå•†ä¸šç”¨é€”

### æŠ€æœ¯æ³¨æ„äº‹é¡¹
- åˆç†è®¾ç½®çˆ¬å–é¢‘ç‡ï¼Œé¿å…å¯¹åšç‰›ç¤¾åŒºæœåŠ¡å™¨é€ æˆå‹åŠ›
- å»ºè®®ä½¿ç”¨ä»£ç†æ± é¿å…IPè¢«å°
- å®šæœŸæ£€æŸ¥ç½‘ç«™ç»“æ„å˜åŒ–ï¼ŒåŠæ—¶æ›´æ–°çˆ¬è™«ä»£ç 
- å»ºè®®åœ¨éé«˜å³°æ—¶æ®µè¿›è¡Œçˆ¬å–

### æ•°æ®è´¨é‡
- çˆ¬å–çš„æ•°æ®å¯èƒ½åŒ…å«HTMLæ ‡ç­¾ï¼Œéœ€è¦è¿›ä¸€æ­¥æ¸…ç†
- å›¾ç‰‡é“¾æ¥å¯èƒ½éœ€è¦é¢å¤–å¤„ç†æ‰èƒ½è®¿é—®
- æ—¶é—´æ ¼å¼å¯èƒ½éœ€è¦ç»Ÿä¸€åŒ–å¤„ç†
- å»ºè®®å®šæœŸéªŒè¯æ•°æ®çš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§

## ğŸ”§ å¼€å‘ç¯å¢ƒ

- **Python**: 3.8+
- **IDE**: æ¨èä½¿ç”¨ PyCharm æˆ– VS Code
- **ç‰ˆæœ¬æ§åˆ¶**: Git
- **åŒ…ç®¡ç†**: pip

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [Python å®˜æ–¹æ–‡æ¡£](https://docs.python.org/)
- [Requests æ–‡æ¡£](https://requests.readthedocs.io/)
- [BeautifulSoup4 æ–‡æ¡£](https://www.crummy.com/software/BeautifulSoup/)
- [Selenium æ–‡æ¡£](https://selenium-python.readthedocs.io/)
- [Scrapy æ–‡æ¡£](https://docs.scrapy.org/)
- [Loguru æ–‡æ¡£](https://loguru.readthedocs.io/)
- [Pydantic æ–‡æ¡£](https://pydantic-docs.helpmanual.io/)
